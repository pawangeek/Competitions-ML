{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import gc\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "def load_embed(file):\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float16')[:1]\n",
    "    \n",
    "    if file == '../input/quoratextemb/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec':\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n",
    "    elif file == '../input/quoratextemb/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin':\n",
    "        embeddings_index = KeyedVectors.load_word2vec_format(file, binary=True)\n",
    "    else:\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n",
    "        \n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def build_vocab(texts):\n",
    "    sentences = texts.progress_apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "def check_coverage(vocab, embeddings_index):\n",
    "    known_words = {}\n",
    "    unknown_words = {}\n",
    "    nb_known_words = 0\n",
    "    nb_unknown_words = 0\n",
    "    for word in vocab.keys():\n",
    "        if word in embeddings_index:\n",
    "            known_words[word] = embeddings_index[word]\n",
    "            nb_known_words += vocab[word]\n",
    "        elif word.capitalize() in embeddings_index:\n",
    "            known_words[word] = embeddings_index[word.capitalize()]\n",
    "            nb_known_words += vocab[word]\n",
    "        elif word.lower() in embeddings_index:\n",
    "            known_words[word] = embeddings_index[word.lower()]\n",
    "            nb_known_words += vocab[word]\n",
    "        elif word.upper() in embeddings_index:\n",
    "            known_words[word] = embeddings_index[word.upper()]\n",
    "            nb_known_words += vocab[word]\n",
    "        else:\n",
    "            unknown_words[word] = vocab[word]\n",
    "            nb_unknown_words += vocab[word]\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n",
    "    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return unknown_words\n",
    "\n",
    "def vocab_check_coverage(train, test):\n",
    "    df = pd.concat([train, test]).reset_index(drop=True)\n",
    "    \n",
    "    vocab = build_vocab(df['text'])\n",
    "    print(\"Glove : \")\n",
    "    oov_glove = check_coverage(vocab, embed_glove)\n",
    "    oov_glove = {\"oov_rate\": len(oov_glove) / len(vocab), 'oov_words': oov_glove}\n",
    "    print(\"Paragram : \")\n",
    "    oov_paragram = check_coverage(vocab, embed_paragram)\n",
    "    oov_paragram = {\"oov_rate\": len(oov_paragram) / len(vocab), 'oov_words': oov_paragram}\n",
    "    print(\"FastText : \")\n",
    "    oov_fasttext = check_coverage(vocab, embed_fasttext)\n",
    "    oov_fasttext = {\"oov_rate\": len(oov_fasttext) / len(vocab), 'oov_words': oov_fasttext}\n",
    "#     print(\"Google : \")\n",
    "#     oov_google = check_coverage(vocab, embed_google)\n",
    "#     oov_google = {\"oov_rate\": len(oov_google) / len(vocab), 'oov_words': oov_google}\n",
    "    \n",
    "    return oov_glove, oov_paragram, oov_fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting GloVe embedding\n",
      "Extracting Paragram embedding\n",
      "Extracting FastText embedding\n"
     ]
    }
   ],
   "source": [
    "glove = '../input/quoratextemb/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "paragram =  '../input/quoratextemb/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "wiki_news = '../input/quoratextemb/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "google_path = '../input/quoratextemb/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "print(\"Extracting GloVe embedding\")\n",
    "embed_glove = load_embed(glove)\n",
    "print(\"Extracting Paragram embedding\")\n",
    "embed_paragram = load_embed(paragram)\n",
    "print(\"Extracting FastText embedding\")\n",
    "embed_fasttext = load_embed(wiki_news)\n",
    "# print(\"Extracting GoogleNews embedding\")\n",
    "# embed_google = load_embed(google_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"../input/innoplexusav/train.csv\")\n",
    "test_df  = pd.read_csv(\"../input/innoplexusav/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2924\n",
      "5279\n"
     ]
    }
   ],
   "source": [
    "print(len(test_df))\n",
    "print(len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_df[['drug','text']]\n",
    "test = test_df[['drug','text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)\n",
    "test['text'] = test['text'].replace(r'http\\S+', '', regex=True).replace(r'www\\S+', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].str.lower()\n",
    "test['text'] = test['text'].str.lower()\n",
    "\n",
    "train['text'] = train['text'].str.replace(r\"\\(.*?\\)\",\"\")\n",
    "test['text'] = test['text'].str.replace(r\"\\(.*?\\)\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\n",
    "    \"Trump's\" : 'trump is',\"'cause\": 'because','√¢‚Ç¨‚Ñ¢': \"'\",',cause': 'because',';cause': 'because',\"ain't\": 'am not','ain,t': 'am not',\n",
    "    'ain;t': 'am not','ain¬¥t': 'am not','ain‚Äôt': 'am not',\"aren't\": 'are not','√¢‚Ç¨‚Äú': '-','√¢‚Ç¨≈ì':'\"',\n",
    "    'aren,t': 'are not','aren;t': 'are not','aren¬¥t': 'are not','aren‚Äôt': 'are not',\"can't\": 'cannot',\"can't've\": 'cannot have','can,t': 'cannot','can,t,ve': 'cannot have',\n",
    "    'can;t': 'cannot','can;t;ve': 'cannot have',\n",
    "    'can¬¥t': 'cannot','can¬¥t¬¥ve': 'cannot have','can‚Äôt': 'cannot','can‚Äôt‚Äôve': 'cannot have',\n",
    "    \"could've\": 'could have','could,ve': 'could have','could;ve': 'could have',\"couldn't\": 'could not',\"couldn't've\": 'could not have','couldn,t': 'could not','couldn,t,ve': 'could not have','couldn;t': 'could not',\n",
    "    'couldn;t;ve': 'could not have','couldn¬¥t': 'could not',\n",
    "    'couldn¬¥t¬¥ve': 'could not have','couldn‚Äôt': 'could not','couldn‚Äôt‚Äôve': 'could not have','could¬¥ve': 'could have',\n",
    "    'could‚Äôve': 'could have',\"didn't\": 'did not','didn,t': 'did not','didn;t': 'did not','didn¬¥t': 'did not',\n",
    "    'didn‚Äôt': 'did not',\"doesn't\": 'does not','doesn,t': 'does not','doesn;t': 'does not','doesn¬¥t': 'does not',\n",
    "    'doesn‚Äôt': 'does not',\"don't\": 'do not','don,t': 'do not','don;t': 'do not','don¬¥t': 'do not','don‚Äôt': 'do not',\n",
    "    \"hadn't\": 'had not',\"hadn't've\": 'had not have','hadn,t': 'had not','hadn,t,ve': 'had not have','hadn;t': 'had not',\n",
    "    'hadn;t;ve': 'had not have','hadn¬¥t': 'had not','hadn¬¥t¬¥ve': 'had not have','hadn‚Äôt': 'had not','hadn‚Äôt‚Äôve': 'had not have',\"hasn't\": 'has not','hasn,t': 'has not','hasn;t': 'has not','hasn¬¥t': 'has not','hasn‚Äôt': 'has not',\n",
    "    \"haven't\": 'have not','haven,t': 'have not','haven;t': 'have not','haven¬¥t': 'have not','haven‚Äôt': 'have not',\"he'd\": 'he would',\n",
    "    \"he'd've\": 'he would have',\"he'll\": 'he will',\n",
    "    \"he's\": 'he is','he,d': 'he would','he,d,ve': 'he would have','he,ll': 'he will','he,s': 'he is','he;d': 'he would',\n",
    "    'he;d;ve': 'he would have','he;ll': 'he will','he;s': 'he is','he¬¥d': 'he would','he¬¥d¬¥ve': 'he would have','he¬¥ll': 'he will',\n",
    "    'he¬¥s': 'he is','he‚Äôd': 'he would','he‚Äôd‚Äôve': 'he would have','he‚Äôll': 'he will','he‚Äôs': 'he is',\"how'd\": 'how did',\"how'll\": 'how will',\n",
    "    \"how's\": 'how is','how,d': 'how did','how,ll': 'how will','how,s': 'how is','how;d': 'how did','how;ll': 'how will',\n",
    "    'how;s': 'how is','how¬¥d': 'how did','how¬¥ll': 'how will','how¬¥s': 'how is','how‚Äôd': 'how did','how‚Äôll': 'how will',\n",
    "    'how‚Äôs': 'how is',\"i'd\": 'i would',\"i'll\": 'i will',\"i'm\": 'i am',\"i've\": 'i have','i,d': 'i would','i,ll': 'i will',\n",
    "    'i,m': 'i am','i,ve': 'i have','i;d': 'i would','i;ll': 'i will','i;m': 'i am','i;ve': 'i have',\"isn't\": 'is not',\n",
    "    'isn,t': 'is not','isn;t': 'is not','isn¬¥t': 'is not','isn‚Äôt': 'is not',\"it'd\": 'it would',\"it'll\": 'it will',\"It's\":'it is',\n",
    "    \"it's\": 'it is','it,d': 'it would','it,ll': 'it will','it,s': 'it is','it;d': 'it would','it;ll': 'it will','it;s': 'it is','it¬¥d': 'it would','it¬¥ll': 'it will','it¬¥s': 'it is',\n",
    "    'it‚Äôd': 'it would','it‚Äôll': 'it will','it‚Äôs': 'it is',\n",
    "    'i¬¥d': 'i would','i¬¥ll': 'i will','i¬¥m': 'i am','i¬¥ve': 'i have','i‚Äôd': 'i would','i‚Äôll': 'i will','i‚Äôm': 'i am',\n",
    "    'i‚Äôve': 'i have',\"let's\": 'let us','let,s': 'let us','let;s': 'let us','let¬¥s': 'let us',\n",
    "    'let‚Äôs': 'let us',\"ma'am\": 'madam','ma,am': 'madam','ma;am': 'madam',\"mayn't\": 'may not','mayn,t': 'may not','mayn;t': 'may not',\n",
    "    'mayn¬¥t': 'may not','mayn‚Äôt': 'may not','ma¬¥am': 'madam','ma‚Äôam': 'madam',\"might've\": 'might have','might,ve': 'might have','might;ve': 'might have',\"mightn't\": 'might not','mightn,t': 'might not','mightn;t': 'might not','mightn¬¥t': 'might not',\n",
    "    'mightn‚Äôt': 'might not','might¬¥ve': 'might have','might‚Äôve': 'might have',\"must've\": 'must have','must,ve': 'must have','must;ve': 'must have',\n",
    "    \"mustn't\": 'must not','mustn,t': 'must not','mustn;t': 'must not','mustn¬¥t': 'must not','mustn‚Äôt': 'must not','must¬¥ve': 'must have',\n",
    "    'must‚Äôve': 'must have',\"needn't\": 'need not','needn,t': 'need not','needn;t': 'need not','needn¬¥t': 'need not','needn‚Äôt': 'need not',\"oughtn't\": 'ought not','oughtn,t': 'ought not','oughtn;t': 'ought not',\n",
    "    'oughtn¬¥t': 'ought not','oughtn‚Äôt': 'ought not',\"sha'n't\": 'shall not','sha,n,t': 'shall not','sha;n;t': 'shall not',\"shan't\": 'shall not',\n",
    "    'shan,t': 'shall not','shan;t': 'shall not','shan¬¥t': 'shall not','shan‚Äôt': 'shall not','sha¬¥n¬¥t': 'shall not','sha‚Äôn‚Äôt': 'shall not',\n",
    "    \"she'd\": 'she would',\"she'll\": 'she will',\"she's\": 'she is','she,d': 'she would','she,ll': 'she will',\n",
    "    'she,s': 'she is','she;d': 'she would','she;ll': 'she will','she;s': 'she is','she¬¥d': 'she would','she¬¥ll': 'she will',\n",
    "    'she¬¥s': 'she is','she‚Äôd': 'she would','she‚Äôll': 'she will','she‚Äôs': 'she is',\"should've\": 'should have','should,ve': 'should have','should;ve': 'should have',\n",
    "    \"shouldn't\": 'should not','shouldn,t': 'should not','shouldn;t': 'should not','shouldn¬¥t': 'should not','shouldn‚Äôt': 'should not','should¬¥ve': 'should have',\n",
    "    'should‚Äôve': 'should have',\"that'd\": 'that would',\"that's\": 'that is','that,d': 'that would','that,s': 'that is','that;d': 'that would',\n",
    "    'that;s': 'that is','that¬¥d': 'that would','that¬¥s': 'that is','that‚Äôd': 'that would','that‚Äôs': 'that is',\"there'd\": 'there had',\n",
    "    \"there's\": 'there is','there,d': 'there had','there,s': 'there is','there;d': 'there had','there;s': 'there is',\n",
    "    'there¬¥d': 'there had','there¬¥s': 'there is','there‚Äôd': 'there had','there‚Äôs': 'there is',\n",
    "    \"they'd\": 'they would',\"they'll\": 'they will',\"they're\": 'they are',\"they've\": 'they have',\n",
    "    'they,d': 'they would','they,ll': 'they will','they,re': 'they are','they,ve': 'they have','they;d': 'they would','they;ll': 'they will','they;re': 'they are',\n",
    "    'they;ve': 'they have','they¬¥d': 'they would','they¬¥ll': 'they will','they¬¥re': 'they are','they¬¥ve': 'they have','they‚Äôd': 'they would','they‚Äôll': 'they will',\n",
    "    'they‚Äôre': 'they are','they‚Äôve': 'they have',\"wasn't\": 'was not','wasn,t': 'was not','wasn;t': 'was not','wasn¬¥t': 'was not',\n",
    "    'wasn‚Äôt': 'was not',\"we'd\": 'we would',\"we'll\": 'we will',\"we're\": 'we are',\"we've\": 'we have','we,d': 'we would','we,ll': 'we will',\n",
    "    'we,re': 'we are','we,ve': 'we have','we;d': 'we would','we;ll': 'we will','we;re': 'we are','we;ve': 'we have',\n",
    "    \"weren't\": 'were not','weren,t': 'were not','weren;t': 'were not','weren¬¥t': 'were not','weren‚Äôt': 'were not','we¬¥d': 'we would','we¬¥ll': 'we will',\n",
    "    'we¬¥re': 'we are','we¬¥ve': 'we have','we‚Äôd': 'we would','we‚Äôll': 'we will','we‚Äôre': 'we are','we‚Äôve': 'we have',\"what'll\": 'what will',\"what're\": 'what are',\"what's\": 'what is',\n",
    "    \"what've\": 'what have','what,ll': 'what will','what,re': 'what are','what,s': 'what is','what,ve': 'what have','what;ll': 'what will','what;re': 'what are',\n",
    "    'what;s': 'what is','what;ve': 'what have','what¬¥ll': 'what will',\n",
    "    'what¬¥re': 'what are','what¬¥s': 'what is','what¬¥ve': 'what have','what‚Äôll': 'what will','what‚Äôre': 'what are','what‚Äôs': 'what is',\n",
    "    'what‚Äôve': 'what have',\"where'd\": 'where did',\"where's\": 'where is','where,d': 'where did','where,s': 'where is','where;d': 'where did',\n",
    "    'where;s': 'where is','where¬¥d': 'where did','where¬¥s': 'where is','where‚Äôd': 'where did','where‚Äôs': 'where is',\n",
    "    \"who'll\": 'who will',\"who's\": 'who is','who,ll': 'who will','who,s': 'who is','who;ll': 'who will','who;s': 'who is',\n",
    "    'who¬¥ll': 'who will','who¬¥s': 'who is','who‚Äôll': 'who will','who‚Äôs': 'who is',\"won't\": 'will not','won,t': 'will not','won;t': 'will not',\n",
    "    'won¬¥t': 'will not','won‚Äôt': 'will not',\"wouldn't\": 'would not','wouldn,t': 'would not','wouldn;t': 'would not','wouldn¬¥t': 'would not',\n",
    "    'wouldn‚Äôt': 'would not',\"you'd\": 'you would',\"you'll\": 'you will',\"you're\": 'you are','you,d': 'you would','you,ll': 'you will',\n",
    "    'you,re': 'you are','you;d': 'you would','you;ll': 'you will',\n",
    "    'you;re': 'you are','you¬¥d': 'you would','you¬¥ll': 'you will','you¬¥re': 'you are','you‚Äôd': 'you would','you‚Äôll': 'you will','you‚Äôre': 'you are',\n",
    "    '¬¥cause': 'because','‚Äôcause': 'because',\"you've\": \"you have\",\"could'nt\": 'could not',\n",
    "    \"havn't\": 'have not',\"here‚Äôs\": \"here is\",'i\"\"m': 'i am',\"i'am\": 'i am',\"i'l\": \"i will\",\"i'v\": 'i have',\"wan't\": 'want',\"was'nt\": \"was not\",\"who'd\": \"who would\",\n",
    "    \"who're\": \"who are\",\"who've\": \"who have\",\"why'd\": \"why would\",\"would've\": \"would have\",\"y'all\": \"you all\",\"y'know\": \"you know\",\"you.i\": \"you i\",\n",
    "    \"your'e\": \"you are\",\"arn't\": \"are not\",\"agains't\": \"against\",\"c'mon\": \"common\",\"doens't\": \"does not\",'don\"\"t': \"do not\",\"dosen't\": \"does not\",\n",
    "    \"dosn't\": \"does not\",\"shoudn't\": \"should not\",\"that'll\": \"that will\",\"there'll\": \"there will\",\"there're\": \"there are\",\n",
    "    \"this'll\": \"this all\",\"u're\": \"you are\", \"ya'll\": \"you all\",\"you'r\": \"you are\",\"you‚Äôve\": \"you have\",\"d'int\": \"did not\",\"did'nt\": \"did not\",\"din't\": \"did not\",\"dont't\": \"do not\",\"gov't\": \"government\",\n",
    "    \"i'ma\": \"i am\",\"is'nt\": \"is not\",\"‚ÄòI\":'I',\n",
    "    '·¥Ä…¥·¥Ö':'and','·¥õ ú·¥á':'the',' ú·¥è·¥ç·¥á':'home','·¥ú·¥ò':'up',' ô è':'by','·¥Ä·¥õ':'at','‚Ä¶and':'and','civilbeat':'civil beat',\\\n",
    "    'TrumpCare':'Trump care','Trumpcare':'Trump care', 'OBAMAcare':'Obama care','·¥Ñ ú·¥á·¥Ñ·¥ã':'check','“ì·¥è Ä':'for','·¥õ ú…™s':'this','·¥Ñ·¥è·¥ç·¥ò·¥ú·¥õ·¥á Ä':'computer',\\\n",
    "    '·¥ç·¥è…¥·¥õ ú':'month','·¥°·¥è Ä·¥ã…™…¥…¢':'working','·¥ä·¥è ô':'job','“ì Ä·¥è·¥ç':'from','S·¥õ·¥Ä Ä·¥õ':'start','gubmit':'submit','CO‚ÇÇ':'carbon dioxide','“ì…™ Äs·¥õ':'first',\\\n",
    "    '·¥á…¥·¥Ö':'end','·¥Ñ·¥Ä…¥':'can',' ú·¥Ä·¥†·¥á':'have','·¥õ·¥è':'to',' ü…™…¥·¥ã':'link','·¥è“ì':'of',' ú·¥è·¥ú Ä ü è':'hourly','·¥°·¥á·¥á·¥ã':'week','·¥á…¥·¥Ö':'end','·¥áx·¥õ Ä·¥Ä':'extra',\\\n",
    "    'G Ä·¥á·¥Ä·¥õ':'great','s·¥õ·¥ú·¥Ö·¥á…¥·¥õs':'student','s·¥õ·¥Ä è':'stay','·¥ç·¥è·¥çs':'mother','·¥è Ä':'or','·¥Ä…¥ è·¥è…¥·¥á':'anyone','…¥·¥á·¥á·¥Ö…™…¥…¢':'needing','·¥Ä…¥':'an','…™…¥·¥Ñ·¥è·¥ç·¥á':'income',\\\n",
    "    ' Ä·¥á ü…™·¥Ä ô ü·¥á':'reliable','“ì…™ Äs·¥õ':'first',' è·¥è·¥ú Ä':'your','s…™…¢…¥…™…¥…¢':'signing',' ô·¥è·¥õ·¥õ·¥è·¥ç':'bottom','“ì·¥è ü ü·¥è·¥°…™…¥…¢':'following','M·¥Ä·¥ã·¥á':'make',\\\n",
    "    '·¥Ñ·¥è…¥…¥·¥á·¥Ñ·¥õ…™·¥è…¥':'connection','…™…¥·¥õ·¥á Ä…¥·¥á·¥õ':'internet','financialpost':'financial post', ' úa·¥†·¥á':' have ', '·¥Ña…¥':' can ', 'Ma·¥ã·¥á':' make ', ' Ä·¥á ü…™a ô ü·¥á':' reliable ', '…¥·¥á·¥á·¥Ö':' need ',\n",
    "    '·¥è…¥ ü è':' only ', '·¥áx·¥õ Äa':' extra ', 'a…¥':' an ', 'a…¥ è·¥è…¥·¥á':' anyone ', 's·¥õa è':' stay ', 'S·¥õa Ä·¥õ':' start', 'SHOPO':'shop',\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_case_words = {'tkis' : 'Tyrosine kinase inhibitors','blogthis':'blog','tnfŒ±':'tumor necrosis factor',\n",
    "                  'jimc':'johannesburg international mail centre','iraes':'immune related adverse events',' ive ':' i have ',\n",
    "                  'pdl1':'programmed death ligand','pd1':'programmed death ligand','5asas':'aminosalicylates',' doc ':' doctor ','üôÇ':'good',\n",
    "                 ' cannot ': ' can not ',' spms ': ' secondary progressive multiple sclerosis ',' aes ': ' adverse events ',\n",
    "                 'saes':'serious adverse events','anti-cd20':'monoclonal antibodies',' mri ': ' magnetic resonance imaging ',\n",
    "                 ' re ':' are ',' dmt ': ' drug ',' jcv ': ' virus ',' chemo ': ' chemotherapy ',\n",
    "                 ' 75mgs ': ' 75 miligrams ','Œ±4Œ≤7':'integrin',' pharmgkb ': ' pharmacogenomics knowledgebase ','\\x80\\x99l':'\"',\n",
    "                ' spms ':' secondary progressive multiple sclerosis ','azd9291':'osimertinib',' mymsteam ': 'multiple sclerosis support ',\n",
    "                 ' the ':' ',' an ':' ',' amultiple ':' multiple ',' nsclc ': ' non small cell lung cancer ',' meds ': ' medicines ',\n",
    "                 'thepowerofpoop':'the power of poop',' asas ':' aspirin ','egfr':'estimated glomerular filtration rate',\n",
    "                  ' alk ': ' lung cancer ',' kras mutation ':' mutation ',' imrt ': ' intensity modulated radiation therapy ',\n",
    "                ' ms ': ' multiple sclerosis ',' c797s ':' mutation cells ',' nhs ': ' national health service',\n",
    "                  'politicalspeak': 'political speak','newsspeak':'news speak','clinicaltrials':'clinical trials',\n",
    "                  'stillstannding':'still standing','naturalreader':'natural reader'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_contraction(x, dic):\n",
    "    for word in dic.keys():\n",
    "        if word in x:\n",
    "            x = x.replace(word, dic[word])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5279/5279 [00:04<00:00, 1216.88it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2924/2924 [00:02<00:00, 1037.42it/s]\n"
     ]
    }
   ],
   "source": [
    "train['text'] = train['text'].progress_apply(lambda x: correct_contraction(x, contraction_mapping))\n",
    "test['text']  = test['text'].progress_apply(lambda x: correct_contraction(x, contraction_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_punct = [\n",
    "    ',', '.', '\"', ':', ')', '(', '!', '?', '|', ';', \"'\", '$', '&',\n",
    "    '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '‚Ä¢',  '~', '@', '¬£',\n",
    "    '¬∑', '_', '{', '}', '¬©', '^', '¬Æ', '`',  '<', '‚Üí', '¬∞', '‚Ç¨', '‚Ñ¢', '‚Ä∫',\n",
    "    '‚ô•', '‚Üê', '√ó', '¬ß', '‚Ä≥', '‚Ä≤', '√Ç', '‚ñà', '¬Ω', '√†', '‚Ä¶', '‚Äú', '‚òÖ', '‚Äù',\n",
    "    '‚Äì', '‚óè', '√¢', '‚ñ∫', '‚àí', '¬¢', '¬≤', '¬¨', '‚ñë', '¬∂', '‚Üë', '¬±', '¬ø', '‚ñæ',\n",
    "    '‚ïê', '¬¶', '‚ïë', '‚Äï', '¬•', '‚ñì', '‚Äî', '‚Äπ', '‚îÄ', '‚ñí', 'Ôºö', '¬º', '‚äï', '‚ñº',\n",
    "    '‚ñ™', '‚Ä†', '‚ñ†', '‚Äô', '‚ñÄ', '¬®', '‚ñÑ', '‚ô´', '‚òÜ', '√©', '¬Ø', '‚ô¶', '¬§', '‚ñ≤',\n",
    "    '√®', '¬∏', '¬æ', '√É', '‚ãÖ', '‚Äò', '‚àû', '‚àô', 'Ôºâ', '‚Üì', '„ÄÅ', '‚îÇ', 'Ôºà', '¬ª',\n",
    "    'Ôºå', '‚ô™', '‚ï©', '‚ïö', '¬≥', '„Éª', '‚ï¶', '‚ï£', '‚ïî', '‚ïó', '‚ñ¨', '‚ù§', '√Ø', '√ò',\n",
    "    '¬π', '‚â§', '‚Ä°', '‚àö', '¬´', '¬ª', '¬¥', '¬∫', '¬æ', '¬°', '¬ß', '¬£', '‚Ç§']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "my_punct = list(string.punctuation)\n",
    "all_punct = list(set(my_punct + extra_punct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_punct.remove('-')\n",
    "#all_punct.remove('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_punc_mappings = {\"‚Äî\": \"-\", \"‚Äì\": \"-\", \"_\": \"-\", '‚Äù': '\"', \"‚Ä≥\": '\"', '‚Äú': '\"', '‚Ä¢': '.', '‚àí': '-',\n",
    "                         \"‚Äô\": \"'\", \"‚Äò\": \"'\", \"¬¥\": \"'\", \"`\": \"'\", '\\u200b': ' ', '\\xa0': ' ','ÿå':'','‚Äû':'',\n",
    "                         '‚Ä¶': ' ... ', '\\ufeff': ''}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacing_punctuation(text):\n",
    "    \"\"\"\n",
    "    add space before and after punctuation and symbols\n",
    "    \"\"\"\n",
    "    for punc in all_punct:\n",
    "        if punc in text:\n",
    "            text = text.replace(punc, f' {punc} ')\n",
    "    return text\n",
    "\n",
    "def clean_special_punctuations(text):\n",
    "    for punc in special_punc_mappings:\n",
    "        if punc in text:\n",
    "            text = text.replace(punc, special_punc_mappings[punc])\n",
    "    # remove_diacritics don¬¥t' ->  'don t'\n",
    "    #text = remove_diacritics(text)\n",
    "    return text\n",
    "\n",
    "def spacing_some_connect_words(text):\n",
    "    \"\"\"\n",
    "    'Whyare' -> 'Why are'\n",
    "    \"\"\"\n",
    "    ori = text\n",
    "    for error in mis_spell_mapping:\n",
    "        if error in text:\n",
    "            text = text.replace(error, mis_spell_mapping[error])\n",
    "\n",
    "    text = re.sub(r\" (W|w)hat+(s)*[A|a]*(p)+ \", \" WhatsApp \", text)\n",
    "    text = re.sub(r\" (W|w)hat\\S \", \" What \", text)\n",
    "    text = re.sub(r\" \\S(W|w)hat \", \" What \", text)\n",
    "    text = re.sub(r\" (W|w)hy\\S \", \" Why \", text)\n",
    "    text = re.sub(r\" \\S(W|w)hy \", \" Why \", text)\n",
    "    text = re.sub(r\" (H|h)ow\\S \", \" How \", text)\n",
    "    text = re.sub(r\" \\S(H|h)ow \", \" How \", text)\n",
    "    text = re.sub(r\" (W|w)hich\\S \", \" Which \", text)\n",
    "    text = re.sub(r\" \\S(W|w)hich \", \" Which \", text)\n",
    "    text = re.sub(r\" (W|w)here\\S \", \" Where \", text)\n",
    "    text = re.sub(r\" \\S(W|w)here \", \" Where \", text)\n",
    "    text = mis_connect_re.sub(r\" \\1 \", text)\n",
    "    text = text.replace(\"What sApp\", ' WhatsApp ')\n",
    "    text = remove_space(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def clean_bad_case_words(text):\n",
    "    for bad_word in bad_case_words:\n",
    "        if bad_word in text:\n",
    "            text = text.replace(bad_word, bad_case_words[bad_word])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    #text = remove_space(text)\n",
    "    text = clean_bad_case_words(text)\n",
    "    #text = spacing_some_connect_words(text)\n",
    "    text = spacing_punctuation(text)\n",
    "    text = clean_special_punctuations(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"text\"] = train[\"text\"].apply(preprocess)\n",
    "test[\"text\"] = test[\"text\"].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].str.replace(r'\\b\\w\\b','').str.replace(r'\\s+', ' ')\n",
    "test['text'] = test['text'].str.replace(r'\\b\\w\\b','').str.replace(r'\\s+', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'].replace({r'[^\\x00-\\x7F]+':''}, regex=True, inplace=True)\n",
    "test['text'].replace({r'[^\\x00-\\x7F]+':''}, regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'].replace({'  ':' '}, regex=True, inplace=True)\n",
    "test['text'].replace({'  ':' '}, regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2924\n",
      "5279\n"
     ]
    }
   ],
   "source": [
    "print(len(test_df))\n",
    "print(len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8203/8203 [00:00<00:00, 18028.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove : \n",
      "Found embeddings for 85.82% of vocab\n",
      "Found embeddings for  99.06% of all text\n",
      "Paragram : \n",
      "Found embeddings for 86.18% of vocab\n",
      "Found embeddings for  99.08% of all text\n",
      "FastText : \n",
      "Found embeddings for 82.03% of vocab\n",
      "Found embeddings for  98.92% of all text\n"
     ]
    }
   ],
   "source": [
    "oov_glove, oov_paragram, oov_fasttext = vocab_check_coverage(train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ocrevus', 1854),\n",
       " ('entyvio', 1197),\n",
       " ('keytruda', 974),\n",
       " ('opdivo', 910),\n",
       " ('tagrisso', 761),\n",
       " ('pembrolizumab', 627),\n",
       " ('nivolumab', 620),\n",
       " ('tecfidera', 540),\n",
       " ('osimertinib', 440),\n",
       " ('ros1', 402),\n",
       " ('vedolizumab', 269),\n",
       " ('alectinib', 241),\n",
       " ('durvalumab', 214),\n",
       " ('atezolizumab', 208),\n",
       " ('dabrafenib', 206),\n",
       " ('uceris', 195),\n",
       " ('trametinib', 191),\n",
       " ('siponimod', 189),\n",
       " ('inflectra', 169),\n",
       " ('proctosigmoiditis', 160),\n",
       " ('mavenclad', 156),\n",
       " ('ceritinib', 144),\n",
       " ('tecentriq', 144),\n",
       " ('upadacitinib', 116),\n",
       " ('ibdsuperheroes', 114),\n",
       " ('brigatinib', 109),\n",
       " ('dcvax', 108),\n",
       " ('imfinzi', 94),\n",
       " ('ileorectal', 86),\n",
       " ('qbtx', 79),\n",
       " ('ozanimod', 79),\n",
       " ('catdander', 78),\n",
       " ('renflexis', 76),\n",
       " ('risankizumab', 70),\n",
       " ('baf312', 67),\n",
       " ('ipoop', 65),\n",
       " ('msers', 62),\n",
       " ('mekinist', 62),\n",
       " ('tafinlar', 60),\n",
       " ('filgotinib', 59),\n",
       " ('delzicol', 56),\n",
       " ('lorlatinib', 55),\n",
       " ('rociletinib', 54),\n",
       " ('zykadia', 53),\n",
       " ('gilotrif', 52),\n",
       " ('necitumumab', 51),\n",
       " ('jpouch', 49),\n",
       " ('serviceengland', 49),\n",
       " ('actrims', 46),\n",
       " ('zinbryta', 46)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov_glove['oov_words'][:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'reply posted for jesszidek . hi jess sorry to read about challenges you are having with your health . you mentioned lot in your post . just want to share some info on few of points . first , know you said that you are scared of humira . humira and other biologics are very successful in reducing symptoms and inducing and maintain disease remission . to reduce your level of fear it can help to learn more about your treatment option . you can learn more about some of your treatment options . to learn more view our understanding ibd medication brochure at : . if you would like to talk , contact help center at 888 - 694 - 8872 or at info @ crohnscolitisfoundation . org'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['text'][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = train['text'].str.replace('# ','')\n",
    "train['text'] = train['text'].str.replace(' - ','')\n",
    "train['text'] = train['text'].str.replace(' : ','')\n",
    "\n",
    "test['text'] = test['text'].str.replace('#','')\n",
    "test['text'] = test['text'].str.replace(' - ','')\n",
    "test['text'] = test['text'].str.replace(' : ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2924\n",
      "5279\n"
     ]
    }
   ],
   "source": [
    "print(len(test_df))\n",
    "print(len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting(data):\n",
    "    new=[]\n",
    "    for sentences in data:\n",
    "        yes = sentences.split(\". \")\n",
    "        new.append(yes) \n",
    "        \n",
    "    return new\n",
    "    \n",
    "train['new_text'] = splitting(train['text'])\n",
    "test['new_text'] = splitting(test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2924\n",
      "5279\n"
     ]
    }
   ],
   "source": [
    "print(len(test_df))\n",
    "print(len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning1(data):\n",
    "    new_text=[]\n",
    "    for sentences in data:\n",
    "        matching = [s for s in sentences if 'reply posted' not in s]\n",
    "        new_text.append(matching)\n",
    "        \n",
    "    return new_text\n",
    "\n",
    "train['new_text'] = cleaning1(train['new_text'])\n",
    "test['new_text'] = cleaning1(test['new_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2924\n",
      "5279\n"
     ]
    }
   ],
   "source": [
    "print(len(test_df))\n",
    "print(len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning2(data):\n",
    "    new_text=[]\n",
    "    for sentences in data:\n",
    "        matching = [s for s in sentences if 'help center' not in s]\n",
    "        new_text.append(matching)\n",
    "        \n",
    "    return new_text\n",
    "\n",
    "train['new_text'] = cleaning2(train['new_text'])\n",
    "test['new_text'] = cleaning2(test['new_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2924\n",
      "5279\n"
     ]
    }
   ],
   "source": [
    "print(len(test_df))\n",
    "print(len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning3(data):\n",
    "    new_text=[]\n",
    "    \n",
    "    for sentences in data:\n",
    "        if(len(sentences)>1):\n",
    "            matching = [s for s in sentences if len(s) >= 15]\n",
    "            new_text.append(matching)\n",
    "            \n",
    "        else:\n",
    "            matching = [s for s in sentences if len(s) >= 2]\n",
    "            new_text.append(matching)\n",
    "        \n",
    "    return new_text\n",
    "\n",
    "train['new_text'] = cleaning3(train['new_text'])\n",
    "test['new_text'] = cleaning3(test['new_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2924\n",
      "5279\n"
     ]
    }
   ],
   "source": [
    "print(len(test_df))\n",
    "print(len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def don(d):\n",
    "    n=[]\n",
    "    for s in d:\n",
    "        res = \".\".join(s)\n",
    "        n.append(res)\n",
    "    return n       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = don(train['new_text'])\n",
    "test['text'] = don(test['new_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(['drug','new_text'],axis=1,inplace=True)\n",
    "test.drop(['drug','new_text'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2924\n",
      "5279\n"
     ]
    }
   ],
   "source": [
    "print(len(test_df))\n",
    "print(len(train_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train[\"drug\"] = train[\"drug\"].apply(preprocess)\n",
    "#drug_list = train['drug'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.drop(['text'],axis=1,inplace=True)\n",
    "test_df.drop(['text'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.merge(train_df,train,left_index=True, right_index=True,how='inner')\n",
    "test = pd.merge(test_df,test,left_index=True, right_index=True,how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train_2.csv',index=False)\n",
    "test.to_csv('test_2.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
